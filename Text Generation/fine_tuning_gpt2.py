# -*- coding: utf-8 -*-
"""Fine tuning GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1axyuKdeoYIXjcuXNj6JBW_aQqfW7hAB2

#**Text Generation with GPT-2**

#1. Setting up the Environment
"""

!pip install transformers
!pip install datasets
!pip install ipykernel
!pip install torch

"""# 2.Data Preparation

**a) Dataset collecting and formatting**
"""

from datasets import load_dataset
import pandas as pd

dataset = load_dataset("csv", data_files="/deeplearning_questions.csv")

df = pd.DataFrame(dataset['train'])
df.head()

"""**b) Tokenization**"""

from transformers import GPT2Tokenizer

print(dataset)

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    # Tokenize descriptions and set labels to be same as input_ids
    tokenized = tokenizer(examples["DESCRIPTION"], truncation=True, padding="max_length", max_length=128)
    tokenized["labels"] = tokenized["input_ids"]  # Set labels to be same as input_ids
    return tokenized

tokenized_datasets = dataset.map(tokenize_function, batched=True)

"""**c) Data splitting**"""

# Split the dataset into training and validation sets
from datasets import DatasetDict

# Create a validation split from the training data
dataset = DatasetDict({
    'train': tokenized_datasets['train'].shuffle(seed=42).select([i for i in list(range(int(0.9 * len(tokenized_datasets['train']))))]),
    'validation': tokenized_datasets['train'].shuffle(seed=42).select([i for i in list(range(int(0.9 * len(tokenized_datasets['train'])), len(tokenized_datasets['train'])))])
})

"""# 3. Fine Tuning GPT 2

**1. Load the Pre-trained GPT-2 Model:**
"""

from transformers import GPT2LMHeadModel, Trainer, TrainingArguments

# Load the pre-trained GPT-2 model
model = GPT2LMHeadModel.from_pretrained('gpt2')

# If you added a padding token, resize the model’s embedding layer to match the tokenizer’s vocabulary size
model.resize_token_embeddings(len(tokenizer))

"""**2. Set Up Training Arguments:**"""

training_args = TrainingArguments(
    output_dir="./results",          # Directory to save model checkpoints
    overwrite_output_dir=True,       # Overwrite the content of the output directory
    num_train_epochs=3,              # Number of training epochs
    per_device_train_batch_size=4,   # Batch size per device during training
    save_steps=500,                  # Save checkpoint every 500 steps
    save_total_limit=2,              # Limit the total number of checkpoints
    prediction_loss_only=True,       # Only return loss in the evaluation
)

"""**3. Set Up the Trainer:**"""

trainer = Trainer(
    model=model,                        # The pre-trained GPT-2 model
    args=training_args,                 # Training arguments
    train_dataset=dataset['train'],     # Training dataset
    eval_dataset=dataset['validation']  # Validation dataset
)

"""**4. Start Training:**"""

trainer.train()

"""**5. Run a small test batch:**"""

# Select a small subset for testing
small_test_dataset = tokenized_datasets["train"].select([0, 1, 2])  # Select first 3 examples

# Update the Trainer to use the small test dataset
trainer.train_dataset = small_test_dataset

# Run training
trainer.train()

"""**4. Check Model Output with Sample Data:**"""

# Load model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Tokenize a sample input
inputs = tokenizer("Example input text", return_tensors="pt")

# Perform a forward pass
outputs = model(**inputs, labels=inputs["input_ids"])
print(outputs.loss)  # Should print the loss value

"""#4. Evaluation"""

results = trainer.evaluate()
print(results)

"""#5. Save the Trained Model"""

model.save_pretrained("./gpt2-finetuned")
tokenizer.save_pretrained("./gpt2-finetuned")

"""#6. Generate Text"""

# Generate text
input_text = " What is tokenization"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate output
output = model.generate(input_ids, max_length=100 , num_return_sequences=1, no_repeat_ngram_size=2)

# Decode and print the generated text
print(tokenizer.decode(output[0], skip_special_tokens=True))